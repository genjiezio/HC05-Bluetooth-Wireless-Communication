### Question for you: tune LDA training process.
Normally, we need to determine one hyper-parameter: the number of topics K.The commonly applied criteria are coherence score, including "c_v","c_npmi","u_mass","c_uci", which you can calculate via CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v') with the function of gensim.models.CoherenceModel. Usually, we prefer higher coherence scores and small number of topics (for better interpretability). Note that we perfer u_mass to be close to zero. However, there is no agreement on which one criterion is the best. You can print out or plot how there criteria change when different Ks are used. Of course, you can also refer to perplexity. 

References
- Text Mining and Topic Models (https://core.ac.uk/download/pdf/143407343.pdf)
- gensim.models.CoherenceModel (https://tedboy.github.io/nlps/generated/generated/gensim.models.CoherenceModel.html)

1. Write a loop to train the LDA with different K, calculat the criteria of coherence score ("c_v","c_npmi","u_mass","c_uci"), plot their change over different Ks. (For now, no need to differentiate the training-validation-testing sets, we will cover this later.) In the training process, no need to specify "passes". Please just leave it as default.
2. Select the best K, rebuild (create a new one instead of update) and retrain the model.
3. With the retrained model, append the document distribution over topics (topic weights) as new columns.

#### 1.calculat the criteria of coherence score

from gensim.models import CoherenceModel
import matplotlib.pyplot as plt

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    coherence_values = {'c_v': [], 'c_npmi': [], 'u_mass': [], 'c_uci': []}
    perplexity_values = []
    model_list = []
    
    for num_topics in range(start, limit, step):
        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, random_state=42)
        model_list.append(model)
        
        coherencemodel_cv = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values['c_v'].append(coherencemodel_cv.get_coherence())
        
        coherencemodel_npmi = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_npmi')
        coherence_values['c_npmi'].append(coherencemodel_npmi.get_coherence())
        
        coherencemodel_umass = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')
        coherence_values['u_mass'].append(coherencemodel_umass.get_coherence())
        
        coherencemodel_cuci = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_uci')
        coherence_values['c_uci'].append(coherencemodel_cuci.get_coherence())
        
        perplexity_values.append(model.log_perplexity(corpus))
    
    return model_list, coherence_values, perplexity_values

# 计算不同 K 值下的模型
start, limit, step = 2, 20, 2
model_list, coherence_values, perplexity_values = compute_coherence_values(dictionary, corpus, texts, limit, start, step)

# 绘制一致性得分和困惑度随 K 值变化的图
x = range(start, limit, step)
plt.figure(figsize=(12, 8))
plt.plot(x, coherence_values['c_v'], label='c_v')
plt.plot(x, coherence_values['c_npmi'], label='c_npmi')
plt.plot(x, coherence_values['u_mass'], label='u_mass')
plt.plot(x, coherence_values['c_uci'], label='c_uci')
plt.plot(x, perplexity_values, label='Perplexity')
plt.xlabel('Number of Topics (K)')
plt.ylabel('Score')
plt.legend(('c_v', 'c_npmi', 'u_mass', 'c_uci', 'Perplexity'), loc='best')
plt.title('Coherence Scores and Perplexity for Different K Values')
plt.show()


#### 2.find best k

import numpy as np

# 计算一致性得分的平均值
average_coherence = np.mean([coherence_values['c_v'], coherence_values['c_npmi'], coherence_values['u_mass'], coherence_values['c_uci']], axis=0)

# 选择平均一致性得分最高的 K 值
best_k_index = np.argmax(average_coherence)
best_k = x[best_k_index]
print(f"Best K: {best_k}")
# 重新创建并训练新的 LDA 模型
best_model = LdaModel(corpus=corpus, num_topics=best_k, id2word=dictionary, random_state=42)

#### 3.retrained model, append the document distribution over topics

# 获取以 'topic_' 开头的列名
columns_to_drop = [col for col in News_sample.columns if col.startswith('topic_')]

# 删除这些上个模型遗留的主题分布列
News_sample = News_sample.drop(columns=columns_to_drop)

# 获取每个文档的主题分布
topic_distribution = [
    dict(best_model.get_document_topics(doc, minimum_probability=0))
    for doc in corpus
]

# 转换为 DataFrame，并填充缺失值为 0
topic_df = pd.DataFrame(topic_distribution).fillna(0)
topic_df.columns = [f'topic_{i}' for i in range(topic_df.shape[1])]
topic_df.index = News_sample.index

# 将主题分布合并到 News_sample
News_sample = pd.concat([News_sample, topic_df], axis=1)

print(News_sample.head())



### Questions for you
The above method relies on a lexicon to detect attitude sentiment, now we try if model based methods gives different results. The snownlp package uses E-commerce comments to build model and the default model load the pre-trained one. For convinent practice, we do not load another comment data or fine tune the pretrained model.
1. Please apply snownlp to calculate the sentiment polarity from temp_News_sample['NewsContent'], append the sentiment as a new column "snow_polarity", print out the dataframe after processing.
2. Draw a barplot for the Fin_polarity and snow_polarity to compare their distribution differences. How do they differ?
3. We want to know topic-wise sentiment, i.e., for each topic, what is the average sentiment polarity expressed in the news. Please calculate the weighted average polarity for each topic, print the weighted 1) Fin_polarity and 2) snow_polarity for each topic, and 3) top 20 words of each topic


#### 1.snownlp

from snownlp import SnowNLP
import pandas as pd

# 计算情感极性并添加到 DataFrame 中
temp_News_sample['snow_polarity'] = temp_News_sample['NewsContent'].apply(lambda x: SnowNLP(x).sentiments)

# 打印处理后的 DataFrame
print(temp_News_sample[['NewsContent', 'Fin_polarity', 'snow_polarity']].head())


#### 2.draw barplot

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.histplot(temp_News_sample['Fin_polarity'], bins=20, kde=True, color='blue', label='Fin_polarity')
sns.histplot(temp_News_sample['snow_polarity'], bins=20, kde=True, color='red', label='snow_polarity')
plt.title('Polarity Distribution Comparison')
plt.xlabel('Polarity')
plt.ylabel('Frequency')
plt.legend()
plt.tight_layout()
plt.show()

#### 3.average polarity

# 获取以 'topic_' 开头的列名
topic_columns = [col for col in temp_News_sample.columns if col.startswith('topic_')]

# 从 temp_News_sample 中提取这些列
topic_df = temp_News_sample[topic_columns]

# 计算每个主题的加权平均极性
weighted_fin_polarity = {}
weighted_snow_polarity = {}

for topic in topic_df.columns:
    weighted_fin_polarity[topic] = (temp_News_sample[topic] * temp_News_sample['Fin_polarity']).sum() / temp_News_sample[topic].sum()
    weighted_snow_polarity[topic] = (temp_News_sample[topic] * temp_News_sample['snow_polarity']).sum() / temp_News_sample[topic].sum()

print("Weighted Fin_polarity for each topic:")
print(weighted_fin_polarity)

print("Weighted snow_polarity for each topic:")
print(weighted_snow_polarity)

# 打印每个主题的前 20 个词
for i in range(best_model.num_topics):
    print(f"Top 20 words for topic {i}:")
    print([word for word, prob in best_model.show_topic(i, topn=20)])

